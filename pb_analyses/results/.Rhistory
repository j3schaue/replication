agg1 = ft %>% filter(!(paper %in% rrrs)) %>% group_by(paper) %>%
summarize(nstudies = n(),
nonreplication = sum(replicated == 0),
nonrep0 = sum(p0 < .05), # original nonreplication determinations
nonrep25 = sum(p25 < .05), # nonreplications according to Q test
nonrep33 = sum(p33 < .05),
nonrep67 = sum(p67 < .05),
mdh0scale = mean(mdh0scale), # MDH on the scale of absolute differences
mdh25scale = mean(mdh25scale),
mdh33scale = mean(mdh33scale),
mdh67scale = mean(mdh67scale))
agg = rbind(aggrrr, agg1)
agg
dagg # confirmed it's only the one RPP experiment that has a few different conclusions.
write.csv(agg, './aggregate/table2.csv', row.names=F)
###----------------------------------------------------------------------###
###----------------------------------------------------------------------###
### Q-test Results
###----------------------------------------------------------------------###
###----------------------------------------------------------------------###
###-------------INCLUDE INITIAL FINDING---------------------------------###
###--------Cohen's d Scale
dtoloadq = grep("outlier", #drop outlier files
grep("(alogna|manylabs)_include.csv", # only include the _d files for algona/manylabs
grep("qtest_fixed_.*_include", list.files(), value=T),
value=T, invert=T),
value=T, invert=T)
dqs = do.call(rbind, lapply(dtoloadq, read.csv)) %>%
mutate(mdh0scale = sqrt(2 * vbar * mdh0 / (k - 1)),
mdh25scale = sqrt(2 * vbar * mdh25 / (k - 1)),
mdh33scale = sqrt(2 * vbar * mdh33 / (k - 1)),
mdh67scale = sqrt(2 * vbar * mdh67 / (k - 1)))
dqs
write.csv(dqs, "./aggregate/qtest_include_full_d.csv", row.names=F)
write.csv(dqs %>% select(paper, experiment, p0, p25, p33, p67, mdh0scale, mdh25scale, mdh33scale, mdh67scale),
"./aggregate/table3d.csv")
###--------Natural Scale
toloadq = grep("outlier", #drop outlier files
grep("(alogna|manylabs)_include_d.csv", # only include the _d files for algona/manylabs
grep("qtest_fixed_.*_include", list.files(), value=T),
value=T, invert=T),
value=T, invert=T)
qs = do.call(rbind, lapply(toloadq, read.csv)) %>%
mutate(mdh0scale = sqrt(2 * vbar * mdh0 / (k - 1)),
mdh25scale = sqrt(2 * vbar * mdh25 / (k - 1)),
mdh33scale = sqrt(2 * vbar * mdh33 / (k - 1)),
mdh67scale = sqrt(2 * vbar * mdh67 / (k - 1)))
qs
write.csv(qs, "./aggregate/qtest_include_full.csv", row.names=F)
###----Compare scales
# qs$experiment == dqs$experiment
sum(as.integer(qs$p0 < .05) != as.integer(dqs$p0 < .05))
sum(as.integer(qs$p25 < .05) != as.integer(dqs$p25 < .05))
sum(as.integer(qs$p33 < .05) != as.integer(dqs$p33 < .05))
sum(as.integer(qs$p67 < .05) != as.integer(dqs$p67 < .05))
summary(abs(qs$p0 - dqs$p0))
summary(abs(qs$p25 - dqs$p25))
summary(abs(qs$p33 - dqs$p33))
summary(abs(qs$p67 - dqs$p67))
# p-values are pretty close to the same. Only a few have a difference large enough to care about, but they don't affect conclusions.
###-------------EXCLUDE INITIAL FINDING---------------------------------###
###--------Cohen's d Scale
dtoloadqexc = grep("outlier", #drop outlier files
grep("(alogna|manylabs)_exclude.csv", # only include the _d files for algona/manylabs
grep("qtest_fixed_.*_exclude", list.files(), value=T),
value=T, invert=T),
value=T, invert=T)
dqsex = do.call(rbind, lapply(dtoloadqexc, read.csv)) %>%
mutate(mdh0scale = sqrt(2 * vbar * mdh0 / (k - 1)),
mdh25scale = sqrt(2 * vbar * mdh25 / (k - 1)),
mdh33scale = sqrt(2 * vbar * mdh33 / (k - 1)),
mdh67scale = sqrt(2 * vbar * mdh67 / (k - 1)))
dqsex
write.csv(dqs, "./aggregate/qtest_exclude_full_d.csv", row.names=F)
write.csv(dqs %>% select(paper, experiment, p0, p25, p33, p67, mdh0scale, mdh25scale, mdh33scale, mdh67scale),
"./aggregate/table3d_exc.csv")
###--------Natural Scale
toloadqexc = grep("outlier", #drop outlier files
grep("(alogna|manylabs)_exclude_d.csv", # only include the _d files for algona/manylabs
grep("qtest_fixed_.*_exclude", list.files(), value=T),
value=T, invert=T),
value=T, invert=T)
qsex = do.call(rbind, lapply(toloadqexc, read.csv)) %>%
mutate(mdh0scale = sqrt(2 * vbar * mdh0 / (k - 1)),
mdh25scale = sqrt(2 * vbar * mdh25 / (k - 1)),
mdh33scale = sqrt(2 * vbar * mdh33 / (k - 1)),
mdh67scale = sqrt(2 * vbar * mdh67 / (k - 1)))
qsex
write.csv(qsex, "./aggregate/qtest_exclude_full.csv", row.names=F)
###----Compare scales
qsex$experiment == dqsex$experiment
###----Compare scales
# qsex$experiment == dqsex$experiment
sum(as.integer(qsex$p0 < .05) != as.integer(dqsex$p0 < .05))
sum(as.integer(qsex$p25 < .05) != as.integer(dqsex$p25 < .05))
sum(as.integer(qsex$p33 < .05) != as.integer(dqsex$p33 < .05))
sum(as.integer(qsex$p67 < .05) != as.integer(dqsex$p67 < .05))
summary(abs(qsex$p0 - dqsex$p0))
summary(abs(qsex$p25 - dqsex$p25))
summary(abs(qsex$p33 - dqsex$p33))
summary(abs(qsex$p67 - dqsex$p67))
###--------Cohen's d Scale
dtoloadol = grep("outlier", #drop outlier files
grep("(alogna|manylabs)_include.csv", # only include the _d files for algona/manylabs
grep("qtest_fixed_.*_include", list.files(), value=T),
value=T),
value=T, invert=T)
dtoloadol
grep("qtest_fixed_.*_include", list.files(), value=T)
###--------Cohen's d Scale
dtoloadol = grep("outlier", #drop outlier files
grep("(alogna|manylabs)_include.csv", # only include the _d files for algona/manylabs
grep("qtest_fixed_.*_include", list.files(), value=T),
value=T, invert=T),
value=T)
dtoloadol
###--------Cohen's d Scale
dtoloadol = grep("outlier", #drop outlier files
grep("(alogna|manylabs)_include_outlier.csv", # only include the _d files for algona/manylabs
grep("qtest_fixed_.*_include", list.files(), value=T),
value=T, invert=T),
value=T)
dtoloadol
dol = do.call(rbind, lapply(dtoloadol, read.csv)) #%>%
dol = lapply(dtoloadol, read.csv) #%>%
lapply(dol, names)
dtoloadol[6]
###------------------------------------------------------------###
###------------------------------------------------------------###
###------------------------------------------------------------###
### ANALYSES OF PPIR REPLICATIONS
###------------------------------------------------------------###
###------------------------------------------------------------###
###------------------------------------------------------------###
# set the working directory to src so we can use relative paths
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(dplyr); library(metafor)
source("../package/replicationTest.R")
source("../package/mdh.R")
source("./misc.R")
paper = 'ppir'
tes = 'd'; vr = 'vd'
# load the data
data = read.csv("../data/ppir.csv") %>%
filter(is.finite(d)) %>%   # weed out infinite estimates
filter(!is.na(d))
experiments = unique(data$experiment)
df_inc = do.call(rbind,
lapply(experiments, FUN=function(exp){
dd = dplyr::filter(data, experiment==exp)
ff = rma.uni(dd[[tes]], dd[[vr]], method='FE')
dd$student = rstudent(ff)$z
dd$standard = rstandard(ff)$z
dd$Qi = leave1out(ff)$Q
dd$tbardot = rep(ff$beta, nrow(dd))
return(dd)
})
)
df_exc = do.call(rbind,
lapply(experiments, FUN=function(exp){
dd = dplyr::filter(data, experiment==exp & site!='original')
ff = rma.uni(dd[[tes]], dd[[vr]], method='FE')
dd$student = rstudent(ff)$z
dd$standard = rstandard(ff)$z
dd$Qi = leave1out(ff)$Q
dd$tbardot = rep(ff$beta, nrow(dd))
return(dd)
})
)
##---Sample sizes
foo = df_inc %>% group_by(experiment) %>%
filter(!(site %in% c('original'))) %>%
summarize(nmin = min(n),
nq1 = quantile(n, .25),
nmed = quantile(n, .5),
nmean = mean(n),
nq3 = quantile(n, .75),
nmax = max(n))
##---Check outlier metrics
df_inc %>%
group_by(experiment) %>%
filter(abs(standard) == max(abs(standard)) | Qi == min(Qi)) %>%
dplyr::select(experiment, site, d, vd, n, standard, Qi)
df_exc %>%
group_by(experiment) %>%
filter(abs(standard) == max(abs(standard)) | Qi == min(Qi)) %>%
dplyr::select(experiment, site, d, vd, n, standard, Qi)
###------------------------------------------------------------###
###------------------------------------------------------------###
### PPIR Comparison
###------------------------------------------------------------###
###------------------------------------------------------------###
#---Comparison Framework--------------------------------
## Take the k-1 replicates and create one synthetic estimator
## using either fixed or random effects meta-analysis.
## Compare the initial finding to this synthetic estimator
# Use a fixed and random effects meta-analysis to combine replications
methods = c('FE', 'DL')
# Set the null hypotheses lambda0 = 0, (k-1)/4, (k-1)/3, 2(k-1)/3
ratios = c(0, 1/4, 1/3, 2/3)
lc = runComparisonAnalyses(data=df_inc, t=tes, v=vr, ratios=ratios, paper=paper,
methods=methods)
for(i in seq(lc)){
write.csv(lc[[i]],
paste0("./results/comparison_", paper, "_", names(lc)[i], ".csv"),
row.names=F)
}
###------------------------------------------------------------###
###------------------------------------------------------------###
### PPIR Heterogenetiy Q-Test
###------------------------------------------------------------###
###------------------------------------------------------------###
###----Include original study-----------------------------------
# Main analysis
fetab_inc = qtest_results(df_inc, ratios=ratios, t=tes, v=vr, paper=paper, verbose=F) %>%
left_join(., distinct(dplyr::select(df_inc, experiment, tbardot)))
fetab_inc
write.csv(fetab_inc,
paste0("./results/qtest_fixed_", paper, "_include.csv"), row.names=F)
# drop largest outlier
tbardot = do.call(rbind, lapply(experiments, FUN=function(expt){
dd = df_inc %>%
filter(experiment == expt) %>%
filter(abs(standard) != max(abs(standard)))
return(data.frame(experiment = expt,
tbardot = rma.uni(dd[[tes]], dd[[vr]], method='FE')$beta[1,1]))
}))
outliers_inc = df_inc %>% group_by(experiment) %>%
filter(abs(standard) == max(abs(standard))) %>%
dplyr::select(experiment, outliersite=site, stdresid=standard, Qi)
fetab_inc_ol = qtest_results(df_inc, ratios=ratios, t=tes, v=vr, paper=paper,
exclude="abs(standard)!=max(abs(standard))") %>%
left_join(tbardot) %>%
left_join(outliers_inc)
fetab_inc_ol
write.csv(fetab_inc_ol,
paste0("./results/qtest_fixed_", paper, "_include_outlier.csv"),
row.names=F)
# ###----Exclude original study-----------------------------------------------
# Main analysis
fetab_exc = qtest_results(df_exc, ratios=ratios, t=tes, v=vr, paper=paper) %>%
left_join(., distinct(dplyr::select(df_exc, experiment, tbardot)))
fetab_exc
write.csv(fetab_exc,
paste0("./results/qtest_fixed_", paper, "_exclude.csv"),
row.names=F)
# drop largest outlier
tbardot = do.call(rbind, lapply(experiments, FUN=function(expt){
dd = df_exc %>%
filter(experiment == expt) %>%
filter(abs(standard) != max(abs(standard)))
return(data.frame(experiment = expt,
tbardot = rma.uni(dd[[tes]], dd[[vr]], method='FE')$beta[1,1]))
}))
outliers_exc = df_exc %>% group_by(experiment) %>%
filter(abs(standard) == max(abs(standard))) %>%
dplyr::select(experiment, outliersite=site, stdresid=standard, Qi)
fetab_exc_ol = qtest_results(df_exc, ratios=ratios, t=tes, v=vr, paper=paper,
exclude="abs(standard)!=max(abs(standard))") %>%
left_join(tbardot) %>%
left_join(outliers_exc)
fetab_exc_ol
write.csv(fetab_exc_ol,
paste0("./results/qtest_fixed_", paper, "_exclude_outlier.csv"),
row.names=F)
###------------------------------------------------------------###
###------------------------------------------------------------###
### PPIR variance components
###------------------------------------------------------------###
###------------------------------------------------------------###
# Estimate variance components with Paule-Mandel and DerSimonian-Laird estimators
methods = c("PM", "DL")
###----Include Original Study--------------------------------------------------
for(mm in methods){# for each method, compute tau^2 for each set of replicates
tab = as.data.frame(do.call(rbind, lapply(experiments, FUN=function(ee){
dd = filter(df_inc, experiment==ee)
ff = confint(rma.uni(yi=dd[[tes]], vi=dd[[vr]], method=mm))$random[1,] # get the estimate and the CI
})))
tab$experiment = experiments
tab$vbar = fetab_inc$vbar
tab$paper = paper
write.csv(dplyr::select(tab, experiment, tau2=estimate, ci.lb, ci.ub, paper),
paste0('./results/', paper, '_vc_include_', mm,'.csv'),
row.names=F)
}
###----Exclude Original Study--------------------------------------------------
for(mm in methods){# for each method, compute tau^2 for each set of replicates
tab = as.data.frame(do.call(rbind, lapply(experiments, FUN=function(ee){
dd = filter(df_exc, experiment==ee & site!='original') # exclude the original study
ff = confint(rma.uni(yi=dd[[tes]], vi=dd[[vr]], method=mm))$random[1,] # get the estimate and the CI
})))
tab$experiment = experiments
tab$vbar = fetab_exc$vbar
tab$paper = paper
write.csv(dplyr::select(tab, experiment, tau2=estimate, ci.lb, ci.ub, paper),
paste0('./results/', paper, '_vc_exclude_', mm,'.csv'),
row.names=F)
}
###------------------------------------------------------------###
###------------------------------------------------------------###
### PPIR Forest Plots
###------------------------------------------------------------###
###------------------------------------------------------------###
plots = list()
for(expt in experiments){
tmpdf = filter(df_exc, experiment==expt) %>%
arrange(as.integer(site != 'original'))
plots[[expt]] = forest(tmpdf[[tes]], tmpdf[[vr]], slab=tmpdf$site)
dev.copy(pdf, paste0("./results/plots/", paper, "_", expt, ".pdf"))
dev.off()
}
###--------Cohen's d Scale
dtoloadol = grep("outlier", #drop outlier files
grep("(alogna|manylabs)_include_outlier.csv", # only include the _d files for algona/manylabs
grep("qtest_fixed_.*_include", list.files(), value=T),
value=T, invert=T),
value=T)
dol = lapply(dtoloadol, read.csv) #%>%
lapply(dol, names)
dol = lapply(dtoloadol, read.csv) #%>%
lapply(dol, names)
# set the working directory to src so we can use relative paths
setwd(paste0(dirname(rstudioapi::getActiveDocumentContext()$path), "/results"))
library(dplyr)
source("../misc.R")
###--------Cohen's d Scale
dtoloadol = grep("outlier", #drop outlier files
grep("(alogna|manylabs)_include_outlier.csv", # only include the _d files for algona/manylabs
grep("qtest_fixed_.*_include", list.files(), value=T),
value=T, invert=T),
value=T)
dol = do.call(rbind, lapply(dtoloadol, read.csv)) #%>%
head(dol)
dol$Q - dol$Qi
dol = do.call(rbind, lapply(dtoloadol, read.csv)) %>%
select(paper, experiment, k, Qi,
p0, p25, p33, p67)
dol
dol = do.call(rbind, lapply(dtoloadol, read.csv)) %>%
select(paper, experiment, k, Qi,
p0, p25, p33, p67) %>%
left_join(dplyr::select(dqs, paper, experiment, Q))
dol
dol = do.call(rbind, lapply(dtoloadol, read.csv)) %>%
select(paper, experiment, k, standard, Qi,
p0, p25, p33, p67) %>%
left_join(dplyr::select(dqs, paper, experiment, Q))
names(read.csv(dtolaodol[1]))
names(read.csv(dtoloa\dol[1]))
names(read.csv(dtoloadol[1]))
dol = do.call(rbind, lapply(dtoloadol, read.csv)) %>%
select(paper, experiment, k, outliersite, stdresit, standard, Qi,
p0, p25, p33, p67, tbardot) %>%
left_join(dplyr::select(dqs, paper, experiment, Q))
dol = do.call(rbind, lapply(dtoloadol, read.csv)) %>%
select(paper, experiment, k, outliersite, stdresid, standard, Qi,
p0, p25, p33, p67, tbardot) %>%
left_join(dplyr::select(dqs, paper, experiment, Q))
dol = do.call(rbind, lapply(dtoloadol, read.csv)) %>%
select(paper, experiment, k, outliersite, stdresid, Qi,
p0, p25, p33, p67, tbardot) %>%
left_join(dplyr::select(dqs, paper, experiment, Q))
dol
dol = do.call(rbind, lapply(dtoloadol, read.csv)) %>%
select(paper, experiment, k, outliersite, stdresid, Qi,
p0, p25, p33, p67, tbardot) %>%
left_join(dplyr::select(dqs, paper, experiment, Q)) %>%
select(paper, experiment, outliersite, stdresit, Q, Qi,
p0, p25, p33, p67, tbardot)
dol = do.call(rbind, lapply(dtoloadol, read.csv)) %>%
select(paper, experiment, k, outliersite, stdresid, Qi,
p0, p25, p33, p67, tbardot) %>%
left_join(dplyr::select(dqs, paper, experiment, Q)) %>%
select(paper, experiment, outliersite, stdresid, Q, Qi,
p0, p25, p33, p67, tbardot)
dol
write.csv(dol, "./aggregate/table4d.csv", row.names=F)
###--------Natural Scale
toloadol = grep("outlier", #drop outlier files
grep("(alogna|manylabs)_include_outlier_d.csv", # only include the _d files for algona/manylabs
grep("qtest_fixed_.*_include", list.files(), value=T),
value=T, invert=T),
value=T)
toloadol
ol = do.call(rbind, lapply(toloadol, read.csv)) %>%
select(paper, experiment, k, outliersite, stdresid, Qi,
p0, p25, p33, p67, tbardot) %>%
left_join(dplyr::select(qs, paper, experiment, Q)) %>%
select(paper, experiment, outliersite, stdresid, Q, Qi,
p0, p25, p33, p67, tbardot)
ol
write.csv(ol, "./aggregate/table4.csv", row.names=F)
###-------Compare scales
ol$experiment == dol$experiment
###-------Compare scales
# ol$experiment == dol$experiment
which(ol$outliersite != dol$outliersite)
which(as.integer(ol$p0 < .05) != as.integer(dol$p0 < .05))
which(as.integer(ol$p25 < .05) != as.integer(dol$p25 < .05))
which(as.integer(ol$p33 < .05) != as.integer(dol$p33 < .05))
which(as.integer(ol$p67 < .05) != as.integer(dol$p67 < .05))
summary(abs(ol$p0 - dol$p0))
summary(abs(ol$p25 - dol$p25))
summary(abs(ol$p33 - dol$p33))
summary(abs(ol$p67 - dol$p67))
###----Compare outlier analysis to full analysis on Cohen's d scale
dol$experiment == dqs$experiment
###----Compare outlier analysis to full analysis on Cohen's d scale
# dol$experiment == dqs$experiment
which(as.integer(dol$p0 < .05) != as.integer(dqs$p0 < .05))
which(as.integer(dol$p25 < .05) != as.integer(dqs$p25 < .05))
which(as.integer(dol$p33 < .05) != as.integer(dqs$p33 < .05))
which(as.integer(dol$p67 < .05) != as.integer(dqs$p67 < .05))
summary(abs(dol$p0 - dqs$p0))
summary(abs(dol$p25 - dqs$p25))
summary(abs(dol$p33 - dqs$p33))
summary(abs(dol$p67 - dqs$p67))
# Compare outlier analysis to full analysis on Cohen's d scale
# dol$experiment == dqs$experiment
which(as.integer(dol$p0 < .05) != as.integer(dqs$p0 < .05))
which(as.integer(dol$p25 < .05) != as.integer(dqs$p25 < .05))
which(as.integer(dol$p33 < .05) != as.integer(dqs$p33 < .05))
which(as.integer(dol$p67 < .05) != as.integer(dqs$p67 < .05))
# Compare outlier analysis to full analysis on Cohen's d scale
# dol$experiment == dqs$experiment
dp0 = which(as.integer(dol$p0 < .05) != as.integer(dqs$p0 < .05))
dp25 = which(as.integer(dol$p25 < .05) != as.integer(dqs$p25 < .05))
dp33 = which(as.integer(dol$p33 < .05) != as.integer(dqs$p33 < .05))
dp67 = which(as.integer(dol$p67 < .05) != as.integer(dqs$p67 < .05))
dol$diff0 = 0; dol$diff0[dp0] = 1
dol$diff25 = 0; dol$diff25[dp25] = 1
dol$diff33 = 0; dol$diff33[dp33] = 1
dol$diff67 = 0; dol$diff67[dp67] = 1
head(dol)
head(dol, 10)
dol %>% slice(unique(c(dp0, dp25, dp33, dp67)))
# get only the rows where hypothesis tests change based on one outlier
write.csv(dol %>% slice(unique(c(dp0, dp25, dp33, dp67))),
"./aggregate/outliers_d_diffs.csv", row.names=F)
###-----------------------INCLUDE INITIAL STUDY--------------------------###
# Use Cohen's d scale
###-----Paule-Mandel Estimator
# dtolaodvcpm =
grep("vc_.*_include_PM", list.files())
###-----------------------INCLUDE INITIAL STUDY--------------------------###
# Use Cohen's d scale
###-----Paule-Mandel Estimator
# dtolaodvcpm =
grep("include_PM", list.files())
###-----------------------INCLUDE INITIAL STUDY--------------------------###
# Use Cohen's d scale
###-----Paule-Mandel Estimator
# dtolaodvcpm =
grep("include_PM", list.files(), value=T)
###-----------------------INCLUDE INITIAL STUDY--------------------------###
# Use Cohen's d scale
###-----Paule-Mandel Estimator
# dtolaodvcpm =
grep("(alogna|manylabs)_vc_include_PM.csv", grep("include_PM", list.files(), value=T), value=T, invert=T)
###-----------------------INCLUDE INITIAL STUDY--------------------------###
# Use Cohen's d scale
###-----Paule-Mandel Estimator
dtolaodvcpm = grep("(alogna|manylabs)_vc_include_PM.csv", grep("include_PM", list.files(), value=T), value=T, invert=T)
dpm = do.call(rbind, lapply(dtoloadvcpm, read.csv))
###-----------------------INCLUDE INITIAL STUDY--------------------------###
# Use Cohen's d scale
###-----Paule-Mandel Estimator
dtolaodvcpm = grep("(alogna|manylabs)_vc_include_PM.csv", grep("include_PM", list.files(), value=T), value=T, invert=T)
dpm = do.call(rbind, lapply(dtolaodvcpm, read.csv))
head(dpm)
dpm = do.call(rbind, lapply(dtolaodvcpm, read.csv)) %>%
left_join(dplyr::select(dqs, paper, experiment, k, p0, p25, p33, p67))
head(dpm)
dpm = do.call(rbind, lapply(dtolaodvcpm, read.csv)) %>%
left_join(dplyr::select(dqs, paper, experiment, k, p0, p25, p33, p67)) %>%
select(paper, experiment, k, tau2, cilb=ci.lb, ciub=ci.ub, p0, p25, p33, p67)
dests = dpm %>%
filter(p33 > .05) # 'replicating studies' are ones who we fail to reject replication for 1/3
dests
nrow(dests)
nrow(dpm)
dpm = do.call(rbind, lapply(dtolaodvcpm, read.csv)) %>%
left_join(dplyr::select(dqs, paper, experiment, k, p0, p25, p33, p67, vbar)) %>%
select(paper, experiment, k, tau2, cilb=ci.lb, ciub=ci.ub, p0, p25, p33, p67, vbar)
dests = dpm %>%
filter(p33 > .05) # 'replicating studies' are ones who we fail to reject replication for 1/3
nrow(dests)
dests
dests$tau2/dests$vbar
summary(dests$tau2/dests$vbar)
summary(dests$ciub/dests$vbar)
summary(sqrt(2*dests$tau2))
summary((2*dests$tau2))
summary(sqrt(2*dests$tau2))
write.csv(dests, '.aggregate/vcs_include_pm.csv', row.names=F)
write.csv(dests, './aggregate/vcs_include_pm.csv', row.names=F)
###-----DerSimonian & Laird Estimator
dtolaodvcdl = grep("(alogna|manylabs)_vc_include_DL.csv", grep("include_DL", list.files(), value=T), value=T, invert=T)
dtolaodvcdl
ddl = do.call(rbind, lapply(dtolaodvcdl, read.csv)) %>%
left_join(dplyr::select(dqs, paper, experiment, k, p0, p25, p33, p67, vbar)) %>%
select(paper, experiment, k, tau2, cilb=ci.lb, ciub=ci.ub, p0, p25, p33, p67, vbar)
destsdl = ddl %>%
filter(p33 > .05) # 'replicating studies' are ones who we fail to reject replication for 1/3
nrow(destsdl)
summary(destsdl$tau2/destsdl$vbar)
summary(destsdl$ciub/destsdl$vbar)
summary(sqrt(2*destsdl$tau2))
summary(dests$tau2/dests$vbar)
summary(dests$ciub/dests$vbar)
summary(sqrt(2*dests$tau2))
write.csv(destsdl, './aggregate/vcs_include_dl.csv', row.names=F)
